1. add multiprocessing to vectorising:

def vectorize_text(text_chunk):
    # Your code to vectorize the text_chunk goes here
    # This function should return the vector representation of the input chunk
    pass

    num_processes = multiprocessing.cpu_count()  # Number of CPU cores
pool = multiprocessing.Pool(processes=num_processes)

text_chunks = [...]  # Your list of text chunks
batch_size = 10  # Adjust based on your data size and available resources
batches = [text_chunks[i:i + batch_size] for i in range(0, len(text_chunks), batch_size)]


result_vectors = pool.map(vectorize_text, batches)
use FAISS .add_embeddings() after to add the embeddings


2.hashing by page, add access by page, by document etc.
3.get summarization and other information (ex. total page count, "what are these documents about?") about the whole documents and about all documents to work
4.start using langchain at every step instead of manually passing the promts to openai api
5.save messages for every user
6. fix the fact, that chunks don't go through page